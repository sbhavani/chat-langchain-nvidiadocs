<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer Engine documentation &mdash; Transformer Engine 0.6.0
 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 

          
          
          <a href="#" class="icon icon-home">
            Transformer Engine
          </a>
              <div class="version">
                0.6.0
-f18e677<br/>
Version select: <select onChange="window.location.href = this.value" onFocus="this.selectedIndex = 0">
    <option value="https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html" selected>Current release</option>
    <option value="https://docs.nvidia.com/deeplearning/transformer-engine/archives/index.html">Older releases</option>
</select>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-menu > p > span.caption-text {
      color: #76b900;
    }

    .wy-menu-vertical p {
      height: 32px;
      line-height: 32px;
      padding: 0 1.618em;
      margin: 12px 0 0;
      display: block;
      font-weight: 700;
      text-transform: uppercase;
      font-size: 85%;
      white-space: nowrap;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 1000px;
    }

    /* override table width restrictions */
    .wy-table-responsive table td, .wy-table-responsive table th {
        /* !important prevents the common CSS stylesheets from
          overriding this as on RTD they are loaded after this stylesheet */
        white-space: normal !important;
    }

    .wy-table-responsive {
        overflow: visible !important;
    }

  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Home</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="installation.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#transformer-engine-in-ngc-containers">Transformer Engine in NGC Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="installation.html#pip-from-github">pip - from GitHub</a><ul>
<li class="toctree-l3"><a class="reference internal" href="installation.html#additional-prerequisites">Additional Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="installation.html#installation-stable-release">Installation (stable release)</a></li>
<li class="toctree-l3"><a class="reference internal" href="installation.html#installation-development-build">Installation (development build)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples/quickstart.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/quickstart.html#Overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/quickstart.html#Let’s-build-a-Transformer-layer!">Let’s build a Transformer layer!</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/quickstart.html#Meet-Transformer-Engine">Meet Transformer Engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/quickstart.html#Fused-TE-Modules">Fused TE Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/quickstart.html#Enabling-FP8">Enabling FP8</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API documentation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/common.html">Common API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/common.html#classes">Classes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/common.html#transformer_engine.common.recipe.Format"><code class="docutils literal notranslate"><span class="pre">Format</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="api/common.html#transformer_engine.common.recipe.DelayedScaling"><code class="docutils literal notranslate"><span class="pre">DelayedScaling</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api/framework.html">Framework-specific API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/pytorch.html">pyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="api/pytorch.html#modules">Modules</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.LayerNorm"><code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.LayerNormLinear"><code class="docutils literal notranslate"><span class="pre">LayerNormLinear</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.LayerNormMLP"><code class="docutils literal notranslate"><span class="pre">LayerNormMLP</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.DotProductAttention"><code class="docutils literal notranslate"><span class="pre">DotProductAttention</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.TransformerLayer"><code class="docutils literal notranslate"><span class="pre">TransformerLayer</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="api/pytorch.html#functions">Functions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.fp8_autocast"><code class="docutils literal notranslate"><span class="pre">fp8_autocast()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="api/pytorch.html#transformer_engine.pytorch.checkpoint"><code class="docutils literal notranslate"><span class="pre">checkpoint()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples and Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/fp8_primer.html">Using FP8 with Transformer Engine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/fp8_primer.html#Introduction-to-FP8">Introduction to FP8</a><ul>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#Structure">Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#Mixed-precision-training---a-quick-introduction">Mixed precision training - a quick introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#Mixed-precision-training-with-FP8">Mixed precision training with FP8</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="examples/fp8_primer.html#id1">Using FP8 with Transformer Engine</a><ul>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#FP8-recipe">FP8 recipe</a></li>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#FP8-autocasting">FP8 autocasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#Handling-backward-pass">Handling backward pass</a></li>
<li class="toctree-l3"><a class="reference internal" href="examples/fp8_primer.html#Precision">Precision</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples/advanced_optimizations.html">Performance Optimizations</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/advanced_optimizations.html#Multi-GPU-training">Multi-GPU training</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/advanced_optimizations.html#Gradient-accumulation-fusion">Gradient accumulation fusion</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/advanced_optimizations.html#FP8-weight-caching">FP8 weight caching</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api/c/index.html">C/C++ API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="api/c/activation.html">activation.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/cast.html">cast.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/gemm.html">gemm.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/layer_norm.html">layer_norm.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/softmax.html">softmax.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/transformer_engine.html">transformer_engine.h</a></li>
<li class="toctree-l2"><a class="reference internal" href="api/c/transpose.html">transpose.h</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Transformer Engine</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Transformer Engine documentation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="transformer-engine-documentation">
<h1>Transformer Engine documentation<a class="headerlink" href="#transformer-engine-documentation" title="Permalink to this heading">¶</a></h1>
<p>Transformer Engine (TE) is a library for accelerating Transformer models on NVIDIA GPUs, including
using 8-bit floating point (FP8) precision on Hopper GPUs, to provide better performance with lower
memory utilization in both training and inference. TE provides a collection of highly optimized
building blocks for popular Transformer architectures and an automatic mixed precision-like API that
can be used seamlessly with your PyTorch code. TE also includes a framework agnostic C++ API that
can be integrated with other deep learning libraries to enable FP8 support for Transformers.</p>
<p>As the number of parameters in Transformer models continues to grow, training and inference for
architectures such as BERT, GPT and T5 becomes very memory and compute intensive. Most deep learning
frameworks train with FP32 by default. This is not essential, however, to achieve full accuracy for
many deep learning models. Using mixed-precision training, which combines single-precision (FP32)
with lower precision (e.g. FP16) format when training a model, results in significant speedups with
minimal differences in accuracy as compared to FP32 training. With the introduction of Hopper GPU
architecture FP8 precision was introduced, which offers improved performance over FP16 with no
degradation in accuracy. Although all major deep learning frameworks support FP16, FP8 support is
not available today.</p>
<p>TE addresses the problem of FP8 support by providing APIs that integrate with popular Large Language
Model (LLM) libraries. It provides python layer (initially supporting pyTorch, with support for more
frameworks in the future) consisting of modules to easily build Transformer layer as well as
framework agnostic library in C++ including structs and kernels needed for FP8 support. Modules
provided by TE internally maintain scaling factors and other values needed for FP8 training, greatly
simplifying for the users.</p>
<p>Transformer Engine in action:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">transformer_engine.pytorch</span> <span class="k">as</span> <span class="nn">te</span>
<span class="kn">from</span> <span class="nn">transformer_engine.common</span> <span class="kn">import</span> <span class="n">recipe</span>

<span class="c1"># Set dimensions.</span>
<span class="n">in_features</span> <span class="o">=</span> <span class="mi">768</span>
<span class="n">out_features</span> <span class="o">=</span> <span class="mi">3072</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">2048</span>

<span class="c1"># Initialize model and inputs.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">te</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># Create FP8 recipe. Note: All input args are optional.</span>
<span class="n">fp8_recipe</span> <span class="o">=</span> <span class="n">recipe</span><span class="o">.</span><span class="n">DelayedScaling</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">fp8_format</span><span class="o">=</span><span class="n">recipe</span><span class="o">.</span><span class="n">Format</span><span class="o">.</span><span class="n">E4M3</span><span class="p">)</span>

<span class="c1"># Enables autocasting for the forward pass</span>
<span class="k">with</span> <span class="n">te</span><span class="o">.</span><span class="n">fp8_autocast</span><span class="p">(</span><span class="n">enabled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fp8_recipe</span><span class="o">=</span><span class="n">fp8_recipe</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<section id="highlights">
<h2>Highlights<a class="headerlink" href="#highlights" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Easy-to-use pyTorch modules enabling building of the Transformer layers with FP8 support on H100
GPUs.</p></li>
<li><p>Optimizations (e.g. fused kernels) for Transformer models across all precisions and NVIDIA GPU
architecures.</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-right" title="Installation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022-2023, NVIDIA CORPORATION &amp; AFFILIATES. All rights reserved..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple)>dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }

  html.writer-html4 .rst-content dl:not(.docutils) .property, html.writer-html5 .rst-content dl[class]:not(.option-list):not(.field-list):not(.footnote):not(.glossary):not(.simple) .property {
    text-transform: capitalize;
    display: inline-block;
    padding-right: 8px;
  }
  </style>

  

</body>
</html>